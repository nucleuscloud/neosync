---
title: The future is Synthetic Data Engineering
description: Our thoughts on why Synthetic Data Engineering is the future
date: 2024-01-15
published: true
image: https://assets.nucleuscloud.com/neosync/blog/what-is-test-data.png
authors:
  - evis
---

# Introduction

There are moments that break the forth wall where you realize that you're living in a time period that is special. That historically, we look back on and can see

I believe that time is now with the advancements being made in Generative AI. Generative AI, is and will continue to, creep into every nook and cranny of our lives. From personal applications like smartphones to business applications like Large Language Models (LLMs).

# Why?

Generative AI and machine learning have gotten so good that we can effectively trick ourselves into believing that synthetic data is real data.

There's a model in machine learning called Generative Adverserial Networks (GANs). A GAN is a model in which two neural networks compete with each other by using using deep learning methods to become more accurate in their predictions. The first model, called a generator, attempts to 'trick' the second model, called a discriminiator, into thinking that the data that the generator created is real. The discriminator is trained on real data and it's job is to, well, discriminate the generator's data and determine if the data is in fact real or fake. After many cycles, the generator eventually creates data that the discriminator can no longer determine is fake because it 'looks' (and I'll define this a little more later) so much like the real training data that the discriminator was trained on.

Now imagine taking the same GAN concept and applying it to an organization. Where you have a team or system (the generator) that is responsible for creating data that 'looks' so much like your production data that other teams in the organization (the discriminators) can't tell that it's synthetic. What would this mean? In a world where developers aren't limited by unstable staging environments, happy-path datasets, flaky tests, data-approval bureaucracy and silos, they can build safer, more reliable applications, faster. All without the security and privacy concerns of a data leak.

Enterprise companies make it too difficult for developers to get access to the data they need. It's not for good reason. Data breaches can cause billions of dollars of tangible costs and potentially more in reputational costs. Here we have fundamental friction. As a developer, I need high quality, realistic data to build and test a feature but the organization doesn't want to give to to me because I may accidently put it into an unsecured [S3 Bucket](capital one leak link). So I have to go through a data-approval process which may take weeks, then the data has to be scrubbed of PII and other sensitive data which may take weeks and then finally at the end of that, I have a data set I can use. Unless of course, it's missing something and then I have to go through the process again. Reasons like this are why it takes enterpruse companies years to ship.

The situation isn't any better in startups and smaller companies. Anyone and everyone has access to production. Want a local copy? Sure thing, just `PGDUMP` from prod and `PGRESTORE` locally. Pretty much anything is justified in the name of shipping fast. Security and privacy be damned.

If we care about data privacy and security (which we all should), we're caught between a rock and a hard place. Enterprises make it too hard to get data because they're risk-averse and startups make it too easy to get data because they need to ship features. Is there a one-size fits all solution?

# What

What is Synthetic Data Engineering? replacing traditional data with synthetic data in all aspects of engineering where real data is not required. This is taditional product engineeering where develoeprs are building features, this is machine learning where ML engineers are using data to glean insights from customers and improve their expreiences. Data engineers who are writing and testing data pipelines shojldn't be doing this with customer data .

- increase developer velocity
- unshackle developers from security and privacy rules that force them to safeguard data, when it's synthetic it's not real data
- better testindag and feasibility
