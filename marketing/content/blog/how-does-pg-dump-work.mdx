---
title: A technical deep dive into how PGDUMP works
description: Learn how PGDUMP works under the covers
date: 2024-03-11
published: true
image: /images/blogs/pgdump.svg
authors:
  - evis
---

# Introduction

PGDUMP (or pg_dump) is a [Postgres utility](https://www.postgresql.org/docs/current/app-pgdump.html) [(source)](https://github.com/postgres/postgres/blob/master/src/bin/pg_dump/pg_dump.c) that comes bundled with every PostgreSQL installation. It is a command-line tool that connects to a PostgreSQL server and constructs queries to extract database metadata and/or table data, including all of the constraints. This exported file can then be used to recreate the database on the same or another PostgreSQL server. This is really useful for backups and data migrations.

Let's take a step-by-step look at exactly how PGDUMP works under the hood

# Step 1 - Connect and Query

When you run the pg_dump command, the first thing it does is establish a connection to the PostgreSQL server using the provided connection string. It uses the same libpq library as the psql command-line client. This is nice because it ensures compatability across versions.

Once connected, pg_dump queries the PostgreSQL system catalogs to retrieve metadata about the target database. but

Once connected, pg_dump queries the PostgreSQL system catalogs to retrieve metadata about the target database. This includes information on schemas, tables, data types, indexes, constraints, views, functions, and more. It fetches all the database object definitions and their properties.

Let's go through these in more detail.

1. **Database Properties**: It queries the `pg_database` system catalog to get general properties of the database like encoding, tablespace, etc.
2. **Extensions**: Queries `pg_extension` to get a list of installed extensions in the database.
3. **Schemas**: Queries `pg_namespace` to get a list of all schemas in the database.
4. **Types**: For each schema, it queries `pg_type` to get a list of data types defined in that schema.
5. **Tables & Columns**: For each schema, it queries `pg_class` and `pg_attribute` to get a list of tables and their column definitions.
6. **Constraints & Indexes:** For each table, it queries `pg_constraint` and `pg_index` to retrieve constraints `(primary keys, foreign keys, etc.)` and index definitions.
7. **Inheritance Hierarchy:** It determines table inheritance hierarchies by examining the `pg_inherits` system catalog.
8. **Views & Sequences:** Queries `pg_class` again to identify views and sequences in each schema.
9. **Functions & Procedures:** Queries `pg_proc` to get functions, procedures, triggers, etc. defined in each schema.
10. **Operators & Operator Classes:** Queries `pg_operator` and `pg_opclass` for user-defined operators and operator classes.
11. **Comments:** Queries `pg_description` to retrieve comments on database objects.

It's important to note that pg_dump doesn't query the tables directly. Instead, it queries the system catalogues which contain all of the metadata representing the database. The order also matters. For example, data types are queried before tables since tables may use custom types. It then buffers this metadata in memory as it gets ready for step 2.

# Step 2: Dump the Data

Now that we have the metadata, pg_dump then starts the process of extracting the raw data for each table in the database. There are configurations for pg_dump where you only get the schema. If so, then this step is effectivelyt skipped. If we want the data as well, then here's a breakdown of what happens:

1. It constructs a query like `SELECT \* FROM schema.table` for each table.
2. This query is executed, and pg_dump fetches the result rows in batches to minimize memory overhead.
3. For each row fetched, pg_dump formats an INSERT statement with the row data like `INSERT INTO schema.table VALUES (...);. Note: pg_dump does this to make it easy to pg_restore.
4. These INSERT statements are buffered in memory in the order the rows were retrieved.

Tables with foreign keys or inheritance hierarchies are dumped in an order that preserves referential integrity. Data is dumped sorted by OID, object identifierm, to maintain consistency across runs.

At this point, pg_dump has, in memory, the contents of the pg_dump output file. Just in time for step 3!

# Step 3: Assembling the Dump File

As pg_dump fetches metadata definitions and table data, it incrementally constructs the dump file by writing out sections in this order:

1. Comments: Any comments on database objects specified.
2. Server Settings: Session settings required to recreate the environment.
3. Extensions: Any installed PostgreSQL extensions used in the database.
4. Type Definitions: Custom data types used in the database.
5. Table Schemas: The SQL statements to create tables, indexes, constraints, etc.
6. Table Data: The INSERT statements populated with each table's data.
7. Procedural Code: Definitions for views, functions, triggers, and other routines.

The output is written as plain text with SQL statements intermixed with PGDUMP directives that indicate which database objects a section applies to.

# Step 4: Output

By default, pg_dump outputs the complete SQL script to standard output (stdout). You can optionally specify an output file to write to instead.

If instructed, pg_dump can also compress the output SQL file using the specified compression format (gzip, bzip2, etc). The compressed file can then be transferred and decompressed on the destination system.

# Example Walkthrough

Let's go through an example of using pg_dump to back up the "dvdrental" sample database:

```bash
$ pg_dump -U postgres dvdrental > dvdrental.sql
```

This command connects to the PostgreSQL server as the "postgres" user and dumps the complete "dvdrental" database as plain text SQL into the file "dvdrental.sql". Alternatively, we could dump it to a plan text file by changing the extension.

Here's a high-level look at what the contents of that file will contain:

```sql
--
-- PostgreSQL database dump
--

-- Dumped from database version 14.1
-- Dumped by pg_dump version 14.1

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: uuid-ossp; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS "uuid-ossp" WITH SCHEMA public;


--
-- Name: EXTENSION "uuid-ossp"; Type: COMMENT; Schema: -; Owner:
--

COMMENT ON EXTENSION "uuid-ossp" IS 'generate universally unique identifiers (UUIDs)';


SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: actor; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.actor (
    actor_id integer NOT NULL,
    first_name character varying(45) NOT NULL,
    last_name character varying(45) NOT NULL,
    last_update timestamp without time zone DEFAULT now() NOT NULL
);

-- ... table schema definitions continue ...

--
-- Data for Name: actor; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.actor (actor_id, first_name, last_name, last_update) FROM stdin;
1	PENELOPE	GUINESS	2006-02-15 04:34:33
2	NICK	DEGENERES	2006-02-15 04:34:33
3	ED	CHASE	2006-02-15 04:34:33
...

-- ... data for other tables ...

--
-- Name: film_actor film_actor_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--
ALTER TABLE ONLY public.film_actor
    ADD CONSTRAINT film_actor_pkey PRIMARY KEY (actor_id, film_id);

-- ... other constraints, indexes, views, functions, etc ...
```

The dump contains the DDL to create all schemas, tables, data types, and other objects first. Then it has the raw INSERT statements populated with data for each table. Finally, it includes the definitions for constraints, indexes, views, functions, and other database objects.

When this dump file is run through psql on another PostgreSQL server, it will accurately reconstruct the complete "dvdrental" database from scratch.

PGDUMP is a powerful and highly configurable utility. While conceptually simple, it employs sophisticated logic and ordering under the hood to accurately capture and reproduce intricate database schemas and raw data. This technical deep dive should give you a solid understanding of the detailed process it uses.

# Wrapping up

# How it works

How PG_DUMP Works

Locking and Consistency: To ensure a consistent snapshot of the database, pg_dump leverages PostgreSQL's MVCC (Multi-Version Concurrency Control) capabilities. This means it can safely back up the database without locking out concurrent users. However, certain operations, like those on large objects, may require brief locks.

Metadata Extraction: pg_dump retrieves metadata about the database structure, including tables, views, indexes, sequences, and other schema objects. This step is crucial for reconstructing the database schema during restoration.

Data Dumping: After extracting the schema, pg_dump proceeds to dump the data. It serializes table data into INSERT statements or COPY commands (depending on the chosen format and user options) to ensure that the data can be accurately restored.

Handling Large Objects: For databases containing large objects (blobs), pg_dump has special handling to ensure these are included in the backup. It typically uses the lo_export function to manage these objects.

Generating Output: The extracted schema and data are then written to the output file. The format of this output can vary:

Plain SQL: A text file containing SQL commands.
Custom Format: A compressed binary format, offering flexibility and smaller file sizes.
Directory Format: Similar to custom format but writes the output into multiple files within a directory. This is useful for parallel dumps and restores.
Tar Format: Creates a tar archive suitable for databases without large objects.
